---
title: "Clustering"
output:
  pdf_document:
    dev: png
---

<!--
cd phdproj/platformMS/
rmarkdown::render('./analyses/cluster.Rmd', 'pdf_document')
-->

```{r codeOpts, echo = F}
# Code chunk option
knitr::opts_chunk$set(echo = F,
                      eval = F)
```

# Cluster analysis

- Cluster analysis
- Dissimilarity metric
- Selecting number of clusters $k$
- Validating clusters (cross-validation)
- Intra-cluster variability
- Inter-cluster variability


We identified regions with similar cumulative exposure patterns, referred to as
threat complexes after [@bowler2018], using a partional *k-medoids* clustering
algorithm, CLARA [CLustering for Large Applications; @kaufman1990].
This algorithm extends the PAM (Partition Around Medoids) algorithm to allow the
analysis of large datasets. The PAM algorithm seeks to identify $k$ objects in the
dataset that are representative of all other objects, *i.e.* medoids (also
referred to as examplars) and that are central to the cluster they represent.
The goal of the algorithm is to iteratively minimize dissimilarity between
objects in the cluster and the representative medoid. Sets of $k$ medoids are
thus selected and replaced in order to iteratively minimize the intra-cluster
dissimilarity.

Because the algorithm identifies clusters by minimizing dissimilarity around a
set of *k* medoids, the algorithm requires a user defined number of clusters
composing the dataset under investigation. When the number of cluster is
unknown, there are strategies to choose the optimal number of clusters. We
compared the


The CLARA algorithm was designed to use with large datasets. It uses the
PAM algorithm on a sample from the original dataset to identify a set of optimal
medoids for the sample, and subsequently assign all objects in the dataset to
the identified clusters. This process is repeated and iterations are compared
on the basis of their average dissimilarity between objects and cluster medoids
to select the optimal set of $k$ medoids that minimizes average dissimilarity.

The number of iterations and the sample size will influence the
quality of the results, so the user must find a balance between the number
of sampling runs, the sample size and analysis time.
<!-- There has to be a reference for this. Look up in the article I already read -->



Analyses were performed using the `cluster` package in R [@machler2018].






## Figures

![](./validation.png)

\newpage

![](./clusterComp.png)

\newpage

![](./indCluster.png)

\newpage

![](./totDiss.png)



- Group according to similarity -> unsupervised machine learning
- k-medoids clustering:
  - minimize variation between points in a cluster
    - Object in the cluster serves as its center
    - Object: representative of the cluster
    - *"The k-medoids approach attempts to identify $k$ objects from the dataset that best represent all objects. Clusters are created by assigning each object in the dataset to its nearest representative (*i.e.* medoid)."*
    - Partition around medoids: PAM
    - Clustering for large applications: CLARA
- Hierarchical clustering
  - *"Organizes states into nested sequences of clusters forming a growing aggregation tres"*
- Partitional clustering:
  - *"Iterativelu performs the classification from randomly predefined initial centroids according to a pregiven number $k$ of clusters"*

# Lexicon

- PAM
- CLARA
- Medoids
- Clustering
- Silhouette
- Cluster
- Similarity / dissimilarity
- Manhattan distance
- Average silhouette width (ASW) (to maximize)
- Davies-Bouldin (DB) index (to minimize)
- Partitional vs hierarchical clustering


# To figure out

- Sample
- Data standardization
- Examplars = medoids?
- Testing with other clustering methods
- Cross-validation
- Bootstraping
- Sum total variation / k -> elbow plot


# Relevant papers

- Long et al. 2010: landscape pattern, CLARA analysis
- Halkidi et al. 2001: Clustering review, number of $k$, silhouette (average silhouette width)
- Kaufman and Rousseeuw 1990: CLARA algorithm developers, data normalization
- Davies-Bouldin 1979: DB index for evaluation of number of $k$
- Bowler et al. 2018: Threat complexes
- Machler et al 2018: Cluster package
- Hamilton et al. 2011: CLARA, multi-beam surveys
- Anderberg 1973: Partitional vs hierarchical clustering
- Cassou et al. 2004: Winter regimes (not very interesting, but discusses clustering)
- Shertzer and Williams 2007: Bootstrap validation, figure with ASW and confidence interval from bootstraping validation

# Relevant links

- http://www.sthda.com/english/wiki/print.php?id=234#partitioning-clustering
- https://github.com/bowlerbear/geographyDrivers/blob/master/analysis.R
- http://cc.oulu.fi/~jarioksa/opetus/metodi/sessio3.pdf
- https://www.rdocumentation.org/packages/cluster/versions/2.0.7-1/topics/clara
- https://uc-r.github.io/kmeans_clustering#silo
-


# Analysis

The time required to perform the analysis grows exponentially with the sample
size, and linearly with the number of iterations. With a dataset composed of
250000 rows and 19 columns, and using a sample size of 10000, an iteration
takes a little over 1.5 minutes to run. This means that the total analysis
would take approximately 2.5 hours for 100 iterations and a sample size of
10000 data points.


Testing: Sample of 10000 data points results in an analysis that takes approximately 1.5 minutes to run.

nS <- number of samples
sS <- sample size


# Code

## Parameters setup

```{r setup}
# Packages
library(magrittr)
library(sf)
library(cluster)
library(vegan)
library(slmeta)
library(graphicsutils)
```


## Data preparation

```{r data}
# Load data
library(slmeta)
data(drivers, package = 'slmeta')
dr <- drivers[, !colnames(drivers) %in% c('UVpositive','UVnegative','ID')]

# Scale data
# Z_{if} = \frac{x_{if} - m_f}{s_f} -> Long et al. 2010
dr <- scale(dr)

# Load other data
load('./analyses/clMed.RData')
load('./analyses/simpDr.RData')
```

## k-mediods clustering

### Clustering

```{r kmedoids}
# Cluster analysis
nSample <- 50
sampleSize <- 10000
k <- 2:10
out <- list('list', length(k))
for(i in k) {
  cat("   k: ", i, "\r")
  out[[i]] <- clara(dr,
                    k = i,
                    sampsize = sampleSize,
                    samples = nSample,
                    metric="manhattan",
                    stand = TRUE,
                    keep.data = FALSE)
}
out[[1]] <- NULL
save(out, file = './analyses/kmedoid.RData')
```

### Validating the number of clusters

#### Average silhouette width

```{r silhouettes1}
# load('./analyses/kmedoid.RData')
# for(i in 1:9) plot(silhouette(out[[i]]))
aswMedoid <- unlist(lapply(out, function(x) x$silinfo$avg.width))
```

#### Elbow method
```{r elbow1}
# Verify number of clusters by using the elbow method,
# i.e. the inflexion point in the sum of squared error (SSE) scree plot
# Extract SSE from each clustering run
# wss function
wss <- function(x) (nrow(x)-1) * sum(apply(x,2,var))

# Measure for each k and each cluster
wssKMedoid <- wss(dr)
for(i in 1:length(out)) { # For each k value tested
  wssTemp <- numeric()
  for(j in 1:k[i]) { # For each cluster
    id <- out[[i]]$clustering == j # Identify observations in cluster j
    wssTemp[j] <- wss(dr[id, ])
  }
  wssKMedoid[k[i]] <- sum(wssTemp)
}
```

## k-means clustering

### Clustering

```{r kmeans}
# K-means cluster analysis
# Stored in a list
out2 <- list('list', length(k))
for(i in k) {
  cat("   k: ", i, "\r")
  out2[[i]] <- kmeans(x = dr,
                      centers = i,
                      iter.max = 1000,
                      nstart = 25,
                      algorithm = 'Lloyd')
}
out2[[1]] <- NULL
save(out2, file = './analyses/kmeans.RData')
```

### Validating the number of clusters

#### Average silhouette width

```{r silhouettes2}
# load('./analyses/.RData')
# Silhouette needs to be computed manually here, because the general function crashes
# This will make a very inefficient function, but one that will work
sil <- function(x, cl) {
  # Euclidian distance function between a single point and another point or a string of points
  euc <- function(x, y) {
    if(is.null(dim(y))) {
      sqrt(sum((x - y) ^ 2))
    } else {
      sqrt(colSums((x - t(y)) ^ 2))
    }
  }

  # Number of rows in dataset x
  nr <- length(cl)

  # Clusters in dataset
  k <- unique(cl)

  # Empty data.frame to store silhouette information
  df <- data.frame(cluster = numeric(nr),
                   neighbor = numeric(nr),
                   sil_width = numeric(nr),
                   stringsAsFactors = F)
  # Calculate silhouette for all data points in dataset x
  for(i in 1:nr) {
    # Cluster of data point i
    df$cluster[i] <- cl[i]

    # ID of points in cluster of point i
    id <- cl == df$cluster[i]

    # a = mean distance of point i to all points within its cluster
      a <- euc(x[i, ], x[id, ]) %>%
           mean()

    # b =  bi is the smallest average distance of i to all points in any other
    #      cluster of which i is not a member.
      # Clusters of which i is not a member
      kb <- k[k != df$cluster[i]]

      # Empty names string to store distances of i to objects in other clusters
      b <- numeric(length(kb))
      names(b) <- kb

      # Distance between i and objects in other clusters
      for(j in 1:length(kb)) {
        # ID of points in first kb cluster
        id <- cl == kb[j]
        b[j] <- euc(x[i, ], x[id, ]) %>%
                mean()
      }

      # Identify neighbour of i, which is the cluster of which i is not a member
      # and with minimum distance to i
      df$neighbor[i] <- as.numeric(names(which.min(b)))

      # b is the average distance of i to its neighbor cluster
      b <- min(b)

      # Evaluate silhouette width: si = (bi - ai) / (max(ai, bi))
      df$sil_width[i] <- (b - a) / max(c(a,b))
  }
  return(df)
}

# Now calculate the silhouette width for all observations
silKM <- vector('list', length(k))
for(i in 1:length(k)) {
  cat("   i: ", i, ' of ', length(k), "\r")
  silKM[[i]] <- sil(dr, out2[[i]]$cluster)
  save(silKM, file = './analyses/silKM.RData')
}

# Next get mean silhouette width for each k value
load('./analyses/silKM.RData')
aswKM <- unlist(lapply(silKM, function(x) mean(x$sil_width)))
```

#### Elbow method
```{r elbow2}
# Verify number of clusters by using the elbow method,
# i.e. the inflexion point in the sum of squared error (SSE) scree plot
# Extract SSE from each clustering run
# The kmeans object already includes the total within-cluster SSE in $tot.withinss
wssKMeans <- wssKMedoid[1]
for(i in k) wssKMeans[i] <- out2[[i-1]]$tot.withinss

```

### Validation

```{r validation}
png('./analyses/validation.png', width = 1280, height = 920, res = 200, pointsize = 7)
layout(matrix(1:4, ncol = 2))
par(mar = c(5,5,2,1))
# k-medoid
  # Silhouette
  plot(aswMedoid ~ k,
       pch = 20,
       ylab = 'Average Silhouette Width',
       type = 'b',
       frame = F,
       xlab = '',
       xlim = c(0,10),
       cex = 1.5,
       cex.axis = .8)
  mtext(side = 3, 'k-medoids clustering', font = 2, line = .75)
  # SSE as a function of the number of clusters k
  plot(x = c(1,k),
       y = wssKMedoid,
       type="b",
       frame = F,
       xlab="Number of Clusters (k)",
       ylab="Total within-clusters sum of squares",
       pch = 20,
       xlim = c(0,10),
       cex = 1.5,
       cex.axis = .8)
# k-means
  # Silhouette
  plot(aswKM ~ k,
       pch = 20,
       frame = F,
       xlim = c(0,10),
       xlab = '',
       ylab = 'Average Silhouette Width',
       type = 'b',
       cex = 1.5,
       cex.axis = .8)
  mtext(side = 3, 'k-means clustering', font = 2, line = .75)
  # SSE as a function of the number of clusters k
  plot(x = c(1,k),
       y = wssKMeans,
       type="b",
       frame = F,
       xlim = c(0,10),
       xlab="Number of Clusters (k)",
       ylab="Total within-clusters sum of squares",
       pch = 20,
       cex = 1.5,
       cex.axis = .8)
dev.off()
```

## Selected k value and clustering

```{r k}
k <- 7
clMed <- out[[k-1]]
clMean <- out2[[k-1]]
```

## k-medoids & k-means comparison

```{r clusComp}
cl1 <- clMed$clustering
cl2 <- clMean$cluster

# Make clusters similar
id1 <- cl2 == 1
id2 <- cl2 == 2
id3 <- cl2 == 3
id4 <- cl2 == 4
id5 <- cl2 == 5
id6 <- cl2 == 6
id7 <- cl2 == 7
cl2[id1] <- 2
cl2[id2] <- 5
cl2[id3] <- 7
cl2[id4] <- 3
cl2[id5] <- 1
cl2[id6] <- 4
cl2[id7] <- 6

# Differences
cl12 <- as.numeric(cl1 == cl2) + 1
corr <- round((sum(cl1==cl2)/length(cl1)*100))

png('./analyses/clusterComp.png', width = 2560, height = 1840, res = 300, pointsize = 6)
par(mfrow = c(2,2))
pal <- colorRampPalette(slmetaPal('platform'))
# K-medoids clustering
cols <- pal(k)[cl1]
plotEGSL('egslSimple')
plot(st_geometry(egslGrid), add = T, col = cols, border = cols, lwd = .1)
plot(st_geometry(egslSimple), col = 'transparent', border = '#25364A', lwd = .75, add = T)
text(x = mean(par('usr')[1:2]), y = par('usr')[4] - 75000, 'k-medoid clustering', cex = 2, font = 2)
# K-means clustering
cols <- pal(k)[cl2]
plotEGSL('egslSimple')
plot(st_geometry(egslGrid), add = T, col = cols, border = cols, lwd = .1)
plot(st_geometry(egslSimple), col = 'transparent', border = '#25364A', lwd = .75, add = T)
text(x = mean(par('usr')[1:2]), y = par('usr')[4] - 75000, 'k-means clustering', cex = 2, font = 2)
# Difference
pal2 <- colorRampPalette(c('#00000000', pal(1)))
cols <- pal2(2)[cl12]
plotEGSL('egslSimple')
plot(st_geometry(egslGrid), add = T, col = cols, border = cols, lwd = .1)
plot(st_geometry(egslSimple), col = 'transparent', border = '#25364A', lwd = .75, add = T)
text(x = mean(par('usr')[1:2]), y = par('usr')[4] - 75000, 'Correspondance', cex = 2, font = 2)
text(x = mean(par('usr')[1:2]), y = par('usr')[4] - 125000, paste0('Cell correspondance: ', corr, '%'), cex = 1.5)
# Legend
cols <- pal(k)
y <- seq(.85, .15, length.out = 7)
yG <- .035
plot0(xlim = c(0,1), ylim = c(0,1))
for(i in 1:k) polygon(x = c(.1,.2,.2,.1), y = c(y[i]-yG, y[i]-yG, y[i]+yG, y[i]+yG), col = cols[i])
text(x = .21, y = y, labels = paste('Cluster', 1:7), cex = 1.5, adj = c(0, .5))
dev.off()

```

## Individual clusters

```{r indGraph}

data(egslGrid)
egsl <- st_centroid(egslGrid) %>%
        cbind(clMed)

png('./analyses/indCluster.png', width = 3840, height = 2760, res = 300, pointsize = 10)
par(mfrow = c(3,3))
for(i in 1:7) {
  cl = i
  id = egsl$clMed == cl
  plotEGSL('egslSimple', borders = '#74747444')
  plot(st_geometry(egsl[id, 2]), cex = .01, add = T, col = '#187962', border = '#187962', pch = 20)
  text(x = mean(par('usr')[1:2]), y = par('usr')[4] - 75000, i, cex = 2)
}
dev.off()
```


## Inter-cluster variability

Not possible to do it in a single batch, so will have to do it iteratively

```{r simper}
# x <- dr
# iter <- 10
# prop <- .02 # .05: ~500s/iter | .01: ~20s/iter | .02: ~80s/iter
# cl <- cl1
subSimper <- function(x, cl, iter = 100, prop = .1, accr = '') {
  # List to store simper results for post-processing
  simpDr <- vector('list', iter)

  # Determine the number of data points to sample from each cluster
    clSub <- ceiling(table(cl) * prop)

  # Repeat the process for the desired number of iterations
  for(i in 1:iter) {
    cat("   i: ", i, ' of ', iter, "\r")

    # Subsampled dataset
    samp <- vector('list', length(clSub))
    for(j in 1:length(clSub)) samp[[j]] <- sample(x = which(cl == j), size = clSub[j])
    samp <- unlist(samp)

    # Run simper analysis on subset of data
    simpDr[[i]] <- simper(x[samp, ], cl[samp])

    # Save file
    save(simpDr, file = paste0('./analyses/simpDr', accr, '.RData'))
  }
  return(simpDr)
}

# Run iterative simper analysis
iter <- 100
prop <- .05
simpDr <- subSimper(dr, clMed, iter = iter, prop = prop, accr = '1')

#
load('./analyses/simpDr1.RData')
x1 <- simpDr
load('./analyses/simpDr2.RData')
x2 <- simpDr
load('./analyses/simpDr3.RData')
x3 <- simpDr
load('./analyses/simpDr4.RData')
x4 <- simpDr
simpDr <- c(x1,x2,x3,x4)
id <- which(unlist(lapply(simpDr, is.null)))
for(i in rev(id)) simpDr[[i]] <- NULL

# Summarize in array format
iter <- length(simpDr)
nList <- length(simpDr[[1]])
varNames <- c('average')
drNames <- colnames(dr)
simp <- vector('list', nList)
names(simp) <- names(simpDr[[1]])
for(i in 1:nList) simp[[i]] <- array(data = 0, dim = c(ncol(dr), 1, iter), dimnames = list(drNames, varNames))

# Extract species, average and sd
for(i in 1:iter) {
  temp <- simpDr[[i]]
  for(j in 1:nList) {
    # simp[[j]][,'species',i] <- temp[[j]]$species
    simp[[j]][,'average',i] <- temp[[j]]$average
    # simp[[j]][,'sd',i] <- temp[[j]]$sd
  }
}

# Summarize the results over all iterations
simpMean <- simpSd <- vector('list', nList)
names(simpMean) <- names(simpSd) <- names(simp)
for(i in 1:nList) simpMean[[i]] <- apply(X = simp[[i]], MARGIN = c(1,2), FUN = mean, na.rm = T) # average of average
for(i in 1:nList) simpSd[[i]] <- apply(X = simp[[i]], MARGIN = c(1,2), FUN = sd, na.rm = T) # average of average
```

```{r interDiss}
# Let's begin with the total intercluster dissimilarity
totDiss <- matrix(0,k,k)
totDiss[lower.tri(totDiss)] <- unlist(lapply(simpMean, sum))
totDiss[upper.tri(totDiss)] <- t(totDiss)[upper.tri(totDiss)]
totDiss[totDiss == 0] <- NA
xmn <- floor(min(totDiss, na.rm = T))
xmx <- ceiling(max(totDiss, na.rm = T))
xr <- (range(xmn,xmx)+abs(xmn)) * 100
totDiss <- rotate(totDiss)

# Plot
png('./analyses/totDiss.png', width = 1000, height = 980, res = 200, pointsize = 7)
mat <- matrix(ncol = 2, data = 1:2)
layout(mat, widths = c(.85,.15))
par(mar = c(2,2,2,2))
plot0(xlim = c(0,k), ylim = c(0,k))
for(x in 1:k) {
  for(y in 1:k) {
    cols <- pal(xr[2])[totDiss[y,x]*100]
    polygon(x = c(x-1,x,x,x-1), y = c(y-1,y-1,y,y), col = cols)
    text(x = x-.5, y = y-.5, round(totDiss[y,x], 2), col = '#bfbfbf')
  }
}
text(x = (1:k)-.5, y = (k:1)-.5, 1:k, cex = 1.5)
mtext(side = 3, 'Total cluster dissimilarity', font = 2, cex = 1.75)
colorBar(colRamp = pal(100), min = 0, max = 1, align = 'vertical', mar = c(10,0,10,3))
dev.off()

```


```{r simperSummary1}
# Driver names vector
nDr <- ncol(dr)
drNames <- rep(rownames(simp[[1]]), each = iter)

# Data frame to store results
simpSummary <- data.frame(Drivers = drNames, stringsAsFactors = F)

# Average dissimilarity contribution values
for(j in 1:nList) {
  y <- numeric(0)
  for(i in 1:nDr) y <- c(y, simp[[j]][i,'average',])
  simpSummary <- cbind(simpSummary, y)
  colnames(simpSummary)[j+1] <- names(simp)[j]
}

# Function to change names of
changeNames <- function(x) {
  drNames <- data.frame(ori = c("Aragonite","aquacultureInv","coastDev","dirHumImpact",
                                "fisheriesDD","fisheriesDNH","fisheriesDNL",
                                "fisheriesPHB","fisheriesPLB","Hypoxia","inorgPol",
                                "invasives", "marinePol","nutrientInput",
                                "orgPol","seaLevel","shipping","sst", "toxicAlgae"),
                        new = c('ARAG','AQUA','CD','DHI','DD','DNH','DNL','PHB','PLB', "HYP",
                                 'IP','INV','MP','NI','OP','SL','SHIP','SST',"TOX"),
                        stringsAsFactors = F)
  for(i in 1:nrow(drNames)) x$Drivers[x$Drivers == drNames$ori[i]] <- drNames$new[i]
  x
}
simpSummary <- changeNames(simpSummary)
# simpSummary$Drivers <- factor(simpSummary$Drivers, drNames$new)

# Figure
# Set layout view
mat <- matrix(0,k,k)
# codens <- mat[upper.tri(mat)] <- 1:sum(upper.tri(mat))
kern <- mat[lower.tri(mat)] <- (max(mat)+1):(sum(lower.tri(mat)) + max(mat))
diagMat <- diag(mat) <- (max(mat)+1):(max(mat)+k)

# Graph
png('./analyses/simper.png', width = 3700, height = 1840, res = 200, pointsize = 15)
layout(mat)
par(mar = c(2,2,0,0))
for(i in 2:ncol(simpSummary)) boxplot(simpSummary[,i] ~ simpSummary$Drivers, cex.axis = .2, frame = F)
for(i in 1:k) { graphicsutils::plot0(); text(0, 0, i, cex = 2) }
dev.off()
```

```{r contDiss}

# Driver contribution to overall dissimilarity
contDiss <- lapply(simpMean, function(x) x[,1] / sum(x[,1])) %>%
            as.data.frame() %>%
            dplyr::mutate(driver = rownames(.), mean = rowMeans(.), sum = rowSums(.)) %>%
            dplyr::select(driver, mean, sum)
```

```{r cumulativeDiss}
# Accronyms
accr <-  c('ARAG','AQUA','CD','DHI','DD','DNH','DNL','PHB','PLB',
         "HYP",'IP','INV','MP','NI','OP','SL','SHIP','SST',"TOX")

# Set layout view
mat <- matrix(0,k,k)
# codens <- mat[upper.tri(mat)] <- 1:sum(upper.tri(mat))
mat[lower.tri(mat)] <- (max(mat)+1):(sum(lower.tri(mat)) + max(mat))
diag(mat) <- (max(mat)+1):(max(mat)+k)
mat <- t(mat)
mat[lower.tri(mat)] <- (max(mat)+1):length(mat)
mat <- t(mat)

# Compute cumulative dissimilarity
# cumDiss <- lapply(simpMean, function(x) sort((x[,1] / sum(x[,1])), decreasing = T))
# Percent similarity
cumDiss <- lapply(simpMean, function(x) (x[,1] / sum(x[,1])))
p1 <- vector('list', length(cumDiss))
for(i in 1:length(cumDiss)) {
  data <- data.frame(individual = accr,
                     group = c('O','C','C','C','O','O','O','O','O','O','C','C','O','C','C','O','O','O','C'),
                     value = round(cumDiss[[i]]*100))
  rownames(data) <- NULL
  p1[[i]] <- circularPlot(data)
}

# Individual clusters
p2 <- vector('list', k)
for(i in 1:length(p2)) {
  id <- egsl$clMed == i

  temp <- egsl %>%
          filter(clMed == i) %>%
          st_coordinates() %>%
          as.data.frame()

  p2[[i]] <- ggplot() +
             geom_sf(data = egslSimple, fill = NA) +
             geom_point(aes(x = X, y = Y), colour = '#187962', size = .01, data = temp) +
             coord_sf(crs = st_crs(egslSimple), datum = NA) +
             theme(
                panel.ontop = TRUE,   ## Note: this is to make the panel grid visible in this example
                panel.grid = element_blank(),
                line = element_blank(),
                rect = element_blank(),
                text = element_blank(),
                plot.background = element_rect(fill = "transparent"))
}


# Total dissimilarity
totDiss <- round(unlist(lapply(simpMean, sum)),2)
names(totDiss) <- NULL
diss <- totDiss*100
diss <- diss - min(diss)
pal3 <- colorRampPalette(c('#c2c2c2', '#494949'))
cols <- pal3(max(diss)+1)[diss+1]
p3 <- vector('list', length(cols))
for(i in 1:length(p3)) {
  datapoly <- data.frame(id = 1, x = c(0,0,1,1), y = c(0,1,1,0))
  p3[[i]] <- ggplot(datapoly, aes(x = x, y = y)) +
              geom_polygon(colour = '#4e4e4e', fill = cols[i]) +
              theme(
                legend.position = "none",
                axis.text = element_blank(),
                axis.title = element_blank(),
                axis.ticks = element_blank(),
                panel.grid = element_blank(),
                panel.background = element_blank(),
                plot.margin = unit(rep(.5,4), "cm")) +
             annotate('text', x = .5, y = .5, label = totDiss[i], size = 6, colour = '#000000')
}

p <- c(p1,p2,p3)

png('./analyses/simper2.png', width = 2000, height = 2000, res = 200, pointsize = 7)
grid.arrange(grobs = p, layout_matrix = mat)
dev.off()
```


## Map of selected cluster

```{r mapCluster}

png('/users/davidbeauchesne/desktop/cluster.png', width = 1280, height = 920, res = 300, pointsize = 10)
pal <- colorRampPalette(slmetaPal('platform'))
# K-medoids clustering
cols <- pal(k)[clMed$clustering]
plotEGSL('egslSimple')
plot(st_geometry(egslGrid), add = T, col = cols, border = cols, lwd = .1)
plot(st_geometry(egslSimple), col = 'transparent', border = '#25364A', lwd = .75, add = T)
dev.off()

```

```{r sim}
# data(egslGrid)
# egsl <- st_centroid(egslGrid) %>%
#         cbind(clMed)
#
k <- 7
sim <- vector('list', k)
# for(i in 1:k) {
for(i in c(2,3,5,6,7,1,4)) {
  cat("   k: ", i, "\r")
  id <- clMed == i
  sim[[i]] <- bootSimper2(dr[id, ], iter = 50, samp = 1200)
  save(sim, file = './analyses/similarity.RData')
}

# Extract species, average and sd
# Summarize in array format
iter = 50
varNames <- c('average')
drNames <- colnames(dr)
simSummary <- vector('list', k)
for(i in 1:k) simSummary[[i]] <- array(data = 0, dim = c(ncol(dr), 1, iter), dimnames = list(drNames, varNames))

for(i in 1:k) {
  # Check if there are multiple iterations or not.
  # If length(sim[[i]]) == 2 it means that there were no iterations and the whole cluster was parsed through the simper2 function
  # If not, then sim[[i]] should be equal to the number of iterations
  if(length(sim[[i]]) == 2) {
    simSummary[[i]] <- matrix(ncol = 1,
                                  data = sim[[i]]$similarityContribution$average,
                                  dimnames = list(drNames, 'average'))
  } else {
    for(j in 1:iter) {
      simSummary[[i]][,'average',j] <- sim[[i]][[j]]$similarityContribution$average
    }
  }
}

# Summarize the results over all iterations
simMean <- vector('list', k)
for(i in 1:k) simMean[[i]] <- apply(X = simSummary[[i]], MARGIN = c(1,2), FUN = mean, na.rm = T) # average of average
# for(i in 1:nList) simpSd[[i]] <- apply(X = simp[[i]], MARGIN = c(1,2), FUN = sd, na.rm = T) # average of average
simTot <- lapply(simMean, sum)





png('./analyses/indCluster.png', width = 3840, height = 2760, res = 300, pointsize = 10)
par(mfrow = c(3,3))
for(i in 1:7) {
  cl = i
  id = egsl$clMed == cl
  plotEGSL('egslSimple', borders = '#74747444')
  plot(st_geometry(egsl[id, 2]), cex = .01, add = T, col = '#187962', border = '#187962', pch = 20)
  text(x = mean(par('usr')[1:2]), y = par('usr')[4] - 75000, i, cex = 2)
}
dev.off()



```
